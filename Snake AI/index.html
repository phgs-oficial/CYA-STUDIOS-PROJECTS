<!DOCTYPE html>
<html lang="pt-BR">
<head>
<meta charset="utf-8" />
<title>Snake AI - CYA NEURAL NETWORK</title>
<link rel="stylesheet" href="style.css">
</head>
<body>
  <h1>Snake AI ‚Äî CYA NEURAL NETWORK</h1>

  <div class="status">
    Vit√≥rias: <span id="wins">0</span> |
    Mortes: <span id="deaths">0</span> |
    treinamentos: <span id="episodes">0</span>
  </div>

  <div class="controls">
    <button id="runBtn" class="btn">Rodar</button>
    <button id="trainBtn" class="btn">Treinar</button>
    <button id="stopBtn" class="btn">Parar</button>
  </div>

  <small>Dica: pressione Treinar para rodar o treino acelerado sem render (muito mais r√°pido). Use Rodar para ver a IA jogando.</small>

  <canvas id="game" width="400" height="400"></canvas>
  <small>este jogo que n√≥s criamos √© totalmente open-source! link: </small>
  <small>Snake: verde<br>ma√ß√£: vermelho</small>
<br><br><br><br><br><br><br>
  <h3>mais como funciona???</h3>
<p>aqui est√° um texto explicativo para voc√™<br>entender.</p>

<img class="imagem-explicativa" src="como funciona.png" alt="como funciona.png">

<section>
        <h2>üéÆ 1. O Ambiente do Jogo Snake</h2>
        <p>Este projeto implementa uma Intelig√™ncia Artificial capaz de jogar Snake sozinha usando <b>Q-Learning</b>, uma t√©cnica cl√°ssica de <b>Reinforcement Learning</b> (Aprendizado por Refor√ßo).<br><br>
        No aprendizado por refor√ßo, o agente aprende por tentativa e erro, recebendo recompensas quando faz coisas boas e puni√ß√µes quando faz coisas ruins.<br><br>
        A IA n√£o recebe nada pronto. Ela aprende sozinha, explorando o ambiente diversas vezes at√© descobrir estrat√©gias eficazes.</p>

        <div class="box">
            A cobrinha possui:<br>
            ‚Ä¢ posi√ß√£o da cabe√ßa<br>
            ‚Ä¢ dire√ß√£o atual<br>
            ‚Ä¢ corpo (lista de segmentos)<br><br>
            A ma√ß√£ aparece em posi√ß√£o aleat√≥ria.<br><br>

            A cada passo a cobra pode:<br>
            ‚Ä¢ continuar em frente<br>
            ‚Ä¢ virar √† esquerda<br>
            ‚Ä¢ virar √† direita<br><br>

            Pegar ma√ß√£ ‚Üí +1 ponto<br>
            Morrer ‚Üí ‚àí1 ponto<br>
            O jogo reinicia automaticamente ap√≥s a morte.
        </div>
    </section>

    <section>
        <h2>üß© 2. Representa√ß√£o do Estado</h2>
        <p>O estado √© convertido em um vetor de 11 valores:</p>

        <div class="box">
            <b>Perigos:</b><br>
            dangerAhead<br>
            dangerLeft<br>
            dangerRight<br><br>

            <b>Dire√ß√£o atual (one-hot):</b><br>
            dirUp, dirRight, dirDown, dirLeft<br><br>

            <b>Posi√ß√£o relativa da fruta:</b><br>
            fruitLeft, fruitRight, fruitUp, fruitDown
        </div>
    </section>

    <section>
        <h2>üß† 3. A Tabela Q</h2>
        <p>O Q-Learning armazena valores para cada combina√ß√£o <code>Q[estado][a√ß√£o]</code>.<br>
        As a√ß√µes s√£o:</p>

        <div class="box">
            STRAIGHT<br>
            LEFT<br>
            RIGHT
        </div>

        <p>A Q-table inicia com todos os valores zerados e √© atualizada conforme a IA joga.</p>
    </section>

    <section>
        <h2>üîÅ 4. Ciclo de Aprendizado</h2>
        <p>A cada passo:</p>
        <div class="box">
            1. Observa o estado atual<br>
            2. Escolhe uma a√ß√£o<br>
            3. O jogo executa<br>
            4. Recebe recompensa e novo estado<br>
            5. Atualiza a Q-table
        </div>
    </section>

    <section>
        <h2>üìå 5. Equa√ß√£o do Q-Learning</h2>

        <div class="box">
            Q(s,a) = Q(s,a) + Œ± * (r + Œ≥ * max(Q(s', *)) ‚àí Q(s,a))
        </div>

        <p>Œ± = learning rate<br>
        Œ≥ = discount factor<br><br>
        Se a a√ß√£o for boa ‚Üí valor sobe.<br>
        Se for ruim ‚Üí valor cai.</p>
    </section>

    <section>
        <h2>üé≤ 6. Exploration vs Exploitation (Epsilon-Greedy)</h2>
        <p>A IA precisa explorar e tamb√©m usar o que j√° aprendeu.</p>

        <div class="box">
            In√≠cio: EPSILON = 1.0 (100% aleat√≥rio)<br>
            Depois: decai gradualmente at√© ~0.05
        </div>
    </section>

    <section>
        <h2>‚ö° 7. Treinamento Acelerado</h2>
        <p>Ao clicar em TREINAR:</p>

        <div class="box">
            ‚Ä¢ Canvas desligado<br>
            ‚Ä¢ V√°rios passos por tick<br>
            ‚Ä¢ Aprendizado ultra r√°pido<br><br>

            C√≥digo usado:<br>
            <code>for (let i=0; i&lt;LOOPS_PER_TICK; i++) { stepTrain(); }</code>
        </div>
    </section>

    <section>
        <h2>üñº 8. Execu√ß√£o Visual</h2>
        <p>No bot√£o RODAR, a IA demonstra o que aprendeu:</p>

        <div class="box">
            ‚Ä¢ Joga em tempo real<br>
            ‚Ä¢ Quase nunca morre<br>
            ‚Ä¢ Pega ma√ß√£s de forma consistente<br>
            ‚Ä¢ Age como um jogador real
        </div>
    </section>

    <section>
        <h2>üß± 9. Sistema de Recompensas</h2>

        <div class="box">
            Comer ma√ß√£ ‚Üí +1<br>
            Morrer ‚Üí ‚àí1<br>
            Movimento normal ‚Üí 0
        </div>

        <p>Regras simples que j√° fazem a IA evoluir com o tempo.</p>
    </section>

    <section>
        <h2>üèÜ 10. Resultado</h2>
        <p>Depois de treinar:</p>

        <div class="box">
            ‚Ä¢ Evita paredes<br>
            ‚Ä¢ Evita o corpo<br>
            ‚Ä¢ Vai na fruta<br>
            ‚Ä¢ Abre espa√ßo para n√£o se trancar<br>
            ‚Ä¢ Sobrevive por longos per√≠odos<br>
            ‚Ä¢ Faz longas sequ√™ncias de frutas
        </div>
    </section>

    <section>
        <h2>üéÅ Conclus√£o</h2>
        <p>Este projeto mostra como uma IA simples pode aprender sozinha com:</p>

        <div class="box">
            Estados simples<br>
            A√ß√µes relativas<br>
            Q-Learning puro<br>
            Recompensas m√≠nimas<br>
            Treinamento acelerado
        </div>
    </section>

<script src="main.js"></script>
</body>
                         </html>
